2022.05.16 17:05:58 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.7d/152454171ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 17:15:46 INFO  es[][o.e.n.Node] stopping ...
2022.05.16 17:15:46 INFO  es[][o.e.n.Node] stopped
2022.05.16 17:15:46 INFO  es[][o.e.n.Node] closing ...
2022.05.16 17:15:46 INFO  es[][o.e.n.Node] closed
2022.05.16 20:28:24 INFO  es[][o.e.n.Node] version[7.17.1], pid[39], build[default/tar/e5acb99f822233d62d6444ce45a4543dc1c8059a/2022-02-23T22:20:54.153567231Z], OS[Linux/5.10.102.1-microsoft-standard-WSL2/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/11.0.14/11.0.14+9-alpine-r0]
2022.05.16 20:28:24 INFO  es[][o.e.n.Node] JVM home [/usr/lib/jvm/java-11-openjdk]
2022.05.16 20:28:25 INFO  es[][o.e.n.Node] JVM arguments [-XX:+UseG1GC, -Djava.io.tmpdir=/opt/sonarqube/temp, -XX:ErrorFile=../logs/es_hs_err_pid%p.log, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djna.tmpdir=/opt/sonarqube/temp, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=COMPAT, -Dcom.redhat.fips=false, -Xmx512m, -Xms512m, -XX:MaxDirectMemorySize=256m, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/sonarqube/elasticsearch, -Des.path.conf=/opt/sonarqube/temp/conf/es, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=false]
2022.05.16 20:28:39 INFO  es[][o.e.p.PluginsService] loaded module [analysis-common]
2022.05.16 20:28:39 INFO  es[][o.e.p.PluginsService] loaded module [lang-painless]
2022.05.16 20:28:39 INFO  es[][o.e.p.PluginsService] loaded module [parent-join]
2022.05.16 20:28:39 INFO  es[][o.e.p.PluginsService] loaded module [reindex]
2022.05.16 20:28:39 INFO  es[][o.e.p.PluginsService] loaded module [transport-netty4]
2022.05.16 20:28:39 INFO  es[][o.e.p.PluginsService] no plugins loaded
2022.05.16 20:28:39 INFO  es[][o.e.e.NodeEnvironment] using [1] data paths, mounts [[/opt/sonarqube/data (/dev/sdb)]], net usable_space [236.2gb], net total_space [250.9gb], types [ext4]
2022.05.16 20:28:39 INFO  es[][o.e.e.NodeEnvironment] heap size [512mb], compressed ordinary object pointers [true]
2022.05.16 20:28:39 INFO  es[][o.e.n.Node] node name [sonarqube], node ID [0KICZHGIR_WEdTCwjG6c4Q], cluster name [sonarqube], roles [data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
2022.05.16 20:28:54 INFO  es[][o.e.t.NettyAllocator] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=512mb}]
2022.05.16 20:28:55 INFO  es[][o.e.i.r.RecoverySettings] using rate limit [40mb] with [default=40mb, read=0b, write=0b, max=0b]
2022.05.16 20:28:55 INFO  es[][o.e.d.DiscoveryModule] using discovery type [zen] and seed hosts providers [settings]
2022.05.16 20:28:55 INFO  es[][o.e.g.DanglingIndicesState] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
2022.05.16 20:28:56 INFO  es[][o.e.n.Node] initialized
2022.05.16 20:28:56 INFO  es[][o.e.n.Node] starting ...
2022.05.16 20:28:56 INFO  es[][o.e.t.TransportService] publish_address {127.0.0.1:39691}, bound_addresses {127.0.0.1:39691}
2022.05.16 20:28:58 WARN  es[][o.e.b.BootstrapChecks] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
2022.05.16 20:28:58 INFO  es[][o.e.c.c.Coordinator] cluster UUID [ibnBxlt0TPG9C3JACSFhpQ]
2022.05.16 20:28:59 INFO  es[][o.e.c.s.MasterService] elected-as-master ([1] nodes joined)[{sonarqube}{0KICZHGIR_WEdTCwjG6c4Q}{t4YqTpChQmmV1Hog8VlPew}{127.0.0.1}{127.0.0.1:39691}{cdfhimrsw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 7, version: 146, delta: master node changed {previous [], current [{sonarqube}{0KICZHGIR_WEdTCwjG6c4Q}{t4YqTpChQmmV1Hog8VlPew}{127.0.0.1}{127.0.0.1:39691}{cdfhimrsw}]}
2022.05.16 20:28:59 INFO  es[][o.e.c.s.ClusterApplierService] master node changed {previous [], current [{sonarqube}{0KICZHGIR_WEdTCwjG6c4Q}{t4YqTpChQmmV1Hog8VlPew}{127.0.0.1}{127.0.0.1:39691}{cdfhimrsw}]}, term: 7, version: 146, reason: Publication{term=7, version=146}
2022.05.16 20:29:01 INFO  es[][o.e.h.AbstractHttpServerTransport] publish_address {127.0.0.1:9001}, bound_addresses {127.0.0.1:9001}
2022.05.16 20:29:01 INFO  es[][o.e.n.Node] started
2022.05.16 20:29:01 INFO  es[][o.e.g.GatewayService] recovered [7] indices into cluster_state
2022.05.16 20:29:04 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[metadatas][0]]]).
2022.05.16 20:29:23 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@25a019d6, interval=1s}] took [6489ms] which is above the warn threshold of [5000ms]
2022.05.16 20:29:23 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.4s/6490ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 20:29:24 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.4s/6489146426ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 20:29:40 WARN  es[][o.e.t.ThreadPool] timer thread slept for [10.9s/10997ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 20:29:40 WARN  es[][o.e.t.ThreadPool] timer thread slept for [10.9s/10996700654ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 20:29:40 WARN  es[][o.e.t.ThreadPool] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$3150/0x0000000100a25040@46685e2a] took [11499ms] which is above the warn threshold of [5000ms]
2022.05.16 20:30:26 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][63][11] duration [3.2s], collections [1]/[3.4s], total [3.2s]/[3.5s], memory [166mb]->[60.4mb]/[512mb], all_pools {[young] [114mb]->[0b]/[0b]}{[old] [42mb]->[49.4mb]/[512mb]}{[survivor] [10mb]->[11mb]/[0b]}
2022.05.16 20:30:26 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][63] overhead, spent [3.2s] collecting in the last [3.4s]
2022.05.16 20:37:20 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][young][459][14] duration [701ms], collections [1]/[1.7s], total [701ms]/[4.3s], memory [84.4mb]->[61.4mb]/[512mb], all_pools {[young] [23mb]->[0b]/[0b]}{[old] [60.4mb]->[60.4mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.16 20:37:20 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][459] overhead, spent [701ms] collecting in the last [1.7s]
2022.05.16 20:41:48 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][726] overhead, spent [569ms] collecting in the last [1s]
2022.05.16 21:18:46 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@25a019d6, interval=1s}] took [6710ms] which is above the warn threshold of [5000ms]
2022.05.16 21:19:21 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][2968][22] duration [3.2s], collections [1]/[3.7s], total [3.2s]/[8.1s], memory [108.4mb]->[61.4mb]/[512mb], all_pools {[young] [47mb]->[0b]/[0b]}{[old] [60.4mb]->[60.4mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.16 21:19:21 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][2968] overhead, spent [3.2s] collecting in the last [3.7s]
2022.05.16 21:23:49 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][3231][23] duration [1.4s], collections [1]/[2.3s], total [1.4s]/[9.6s], memory [84.4mb]->[61.3mb]/[512mb], all_pools {[young] [23mb]->[0b]/[0b]}{[old] [60.4mb]->[60.3mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.16 21:23:49 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][3231] overhead, spent [1.4s] collecting in the last [2.3s]
2022.05.16 21:26:02 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.2s/5266ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 21:26:08 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.2s/5266377038ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 21:26:09 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.2s/7261ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 21:26:09 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.2s/7260981124ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 21:26:18 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@717639f3, interval=5s}] took [12311ms] which is above the warn threshold of [5000ms]
2022.05.16 21:26:31 WARN  es[][o.e.t.ThreadPool] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$3150/0x0000000100a25040@27be75f8] took [6599ms] which is above the warn threshold of [5000ms]
2022.05.16 21:26:33 WARN  es[][o.e.h.AbstractHttpServerTransport] handling request [null][GET][/_cluster/health?master_timeout=30s&level=cluster&timeout=30s][Netty4HttpChannel{localAddress=/127.0.0.1:9001, remoteAddress=/127.0.0.1:56770}] took [6800ms] which is above the warn threshold of [5000ms]
2022.05.16 21:50:53 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][4809] overhead, spent [377ms] collecting in the last [1s]
2022.05.16 22:15:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.4s/6400ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:02 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.4s/6400041166ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:00 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@717639f3, interval=5s}] took [6400ms] which is above the warn threshold of [5000ms]
2022.05.16 22:16:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [11.1s/11175ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:04 WARN  es[][o.e.t.ThreadPool] timer thread slept for [11.1s/11174858798ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:12 WARN  es[][o.e.t.ThreadPool] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$3150/0x0000000100a25040@70d675eb] took [7504ms] which is above the warn threshold of [5000ms]
2022.05.16 22:16:18 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.9s/5964ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:20 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.9s/5963309717ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:52 WARN  es[][o.e.t.ThreadPool] timer thread slept for [30.4s/30486ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:53 WARN  es[][o.e.t.ThreadPool] timer thread slept for [30.4s/30486036775ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.16 22:16:52 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@25a019d6, interval=1s}] took [30486ms] which is above the warn threshold of [5000ms]
2022.05.16 22:16:53 WARN  es[][o.e.h.AbstractHttpServerTransport] handling request [null][GET][/_cluster/health?master_timeout=30s&level=cluster&timeout=30s][Netty4HttpChannel{localAddress=/127.0.0.1:9001, remoteAddress=/127.0.0.1:56770}] took [34857ms] which is above the warn threshold of [5000ms]
2022.05.16 22:19:23 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][young][6446][35] duration [871ms], collections [1]/[1.3s], total [871ms]/[11s], memory [88.4mb]->[61.4mb]/[512mb], all_pools {[young] [27mb]->[0b]/[0b]}{[old] [60.4mb]->[60.4mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.16 22:19:23 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][6446] overhead, spent [871ms] collecting in the last [1.3s]
