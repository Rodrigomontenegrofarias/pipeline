2022.05.13 02:02:34 WARN  es[][o.e.t.ThreadPool] timer thread slept for [59.9m/3598962ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 02:02:34 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [3598961ms] which is above the warn threshold of [5000ms]
2022.05.13 02:02:34 WARN  es[][o.e.t.ThreadPool] timer thread slept for [59.9m/3598961801845ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 02:45:55 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][7209][12] duration [1.5s], collections [1]/[1.5s], total [1.5s]/[1.7s], memory [354.5mb]->[62mb]/[512mb], all_pools {[young] [292mb]->[0b]/[0b]}{[old] [49.5mb]->[50mb]/[512mb]}{[survivor] [13mb]->[12mb]/[0b]}
2022.05.13 02:45:55 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][7209] overhead, spent [1.5s] collecting in the last [1.5s]
2022.05.13 02:48:25 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][7359] overhead, spent [346ms] collecting in the last [1s]
2022.05.13 03:02:43 WARN  es[][o.e.t.ThreadPool] timer thread slept for [28s/28059ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:02:43 WARN  es[][o.e.t.ThreadPool] timer thread slept for [28s/28059219765ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:03:12 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.4s/6411ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:04:50 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.4s/6411047566ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:04:45 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [6411ms] which is above the warn threshold of [5000ms]
2022.05.13 03:04:50 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.6m/99344ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:04:50 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.6m/99343860581ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:05:21 WARN  es[][o.e.t.ThreadPool] timer thread slept for [30.4s/30454ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:05:30 WARN  es[][o.e.t.ThreadPool] timer thread slept for [30.4s/30454661613ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:05:34 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.2s/13229ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:05:34 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.2s/13228457360ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:05:34 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6dcabc02, interval=5s}] took [13228ms] which is above the warn threshold of [5000ms]
2022.05.13 03:06:13 WARN  es[][o.e.t.ThreadPool] timer thread slept for [39.5s/39587ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:06:13 WARN  es[][o.e.t.ThreadPool] timer thread slept for [39.5s/39586741402ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:06:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.1s/9128ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:06:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.1s/9128345553ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:08 WARN  es[][o.e.t.ThreadPool] timer thread slept for [45s/45037ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:08 WARN  es[][o.e.t.ThreadPool] timer thread slept for [45s/45036397024ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:08 WARN  es[][o.e.t.ThreadPool] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$3155/0x0000000100a26040@370dc3c5] took [93985ms] which is above the warn threshold of [5000ms]
2022.05.13 03:07:17 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.1s/9100ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:17 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9s/9099233917ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:24 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7s/7060ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:24 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7s/7060199199ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:07:30 WARN  es[][o.e.m.f.FsHealthService] health check of [/opt/sonarqube/data/es7/nodes/0] took [22282ms] which is above the warn threshold of [5s]
2022.05.13 03:08:07 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.8s/6868ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:08:07 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.8s/6867948649ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:08:13 WARN  es[][o.e.h.AbstractHttpServerTransport] handling request [null][GET][/_cluster/health?master_timeout=30s&level=cluster&timeout=30s][Netty4HttpChannel{localAddress=/127.0.0.1:9001, remoteAddress=/127.0.0.1:33532}] took [25682ms] which is above the warn threshold of [5000ms]
2022.05.13 03:08:28 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.8s/6866ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:08:28 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.8s/6866035325ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:08:41 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.8s/5841ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:08:41 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.8s/5840732622ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:16 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [15876ms] which is above the warn threshold of [5000ms]
2022.05.13 03:09:16 WARN  es[][o.e.t.ThreadPool] timer thread slept for [15.8s/15876ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:16 WARN  es[][o.e.t.ThreadPool] timer thread slept for [15.8s/15876103233ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:32 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.3s/6311ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:32 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.3s/6310793570ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:32 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@38f464d8, interval=5s}] took [6529ms] which is above the warn threshold of [5000ms]
2022.05.13 03:09:53 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.5s/7548ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:53 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.5s/7548619447ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:59 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.1s/6128ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:59 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.1s/6127176480ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:09:59 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [6127ms] which is above the warn threshold of [5000ms]
2022.05.13 03:10:35 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.5s/5511ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:10:35 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6dcabc02, interval=5s}] took [5511ms] which is above the warn threshold of [5000ms]
2022.05.13 03:10:35 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.5s/5511116697ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:11:28 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [7082ms] which is above the warn threshold of [5000ms]
2022.05.13 03:11:41 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [10840ms] which is above the warn threshold of [5000ms]
2022.05.13 03:12:20 WARN  es[][o.e.t.ThreadPool] timer thread slept for [21.8s/21851ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:12:20 WARN  es[][o.e.t.ThreadPool] timer thread slept for [21.8s/21850086292ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:12:21 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [22860ms] which is above the warn threshold of [5000ms]
2022.05.13 03:12:29 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.6s/5683ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:12:29 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.6s/5683140350ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:12:29 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5683ms] which is above the warn threshold of [5000ms]
2022.05.13 03:12:56 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6s/6020ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:12:57 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [6020ms] which is above the warn threshold of [5000ms]
2022.05.13 03:12:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6s/6020573425ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:11 WARN  es[][o.e.t.ThreadPool] timer thread slept for [10.5s/10518ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:11 WARN  es[][o.e.t.ThreadPool] timer thread slept for [10.5s/10518315789ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:18 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [17640ms] which is above the warn threshold of [5000ms]
2022.05.13 03:13:27 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.7s/5751ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:27 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.7s/5751538790ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:33 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.8s/5876ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:33 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.8s/5876241760ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:33 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5876ms] which is above the warn threshold of [5000ms]
2022.05.13 03:13:40 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.2s/6257ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:40 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.2s/6256788571ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:13:51 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5418ms] which is above the warn threshold of [5000ms]
2022.05.13 03:14:12 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [6592ms] which is above the warn threshold of [5000ms]
2022.05.13 03:14:20 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5417ms] which is above the warn threshold of [5000ms]
2022.05.13 03:14:33 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.1s/6104ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:14:33 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.1s/6103769397ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:15:00 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.6s/7694ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:15:00 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6dcabc02, interval=5s}] took [12444ms] which is above the warn threshold of [5000ms]
2022.05.13 03:15:00 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.6s/7694512770ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:18:59 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [13080ms] which is above the warn threshold of [5000ms]
2022.05.13 03:18:59 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13s/13080ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:18:59 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13s/13080028243ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:19:35 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5746ms] which is above the warn threshold of [5000ms]
2022.05.13 03:19:55 WARN  es[][o.e.t.ThreadPool] timer thread slept for [20.4s/20472ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:19:55 WARN  es[][o.e.t.ThreadPool] execution of [org.elasticsearch.cluster.InternalClusterInfoService$RefreshScheduler$$Lambda$3155/0x0000000100a26040@35f2a654] took [20471ms] which is above the warn threshold of [5000ms]
2022.05.13 03:19:55 WARN  es[][o.e.t.ThreadPool] timer thread slept for [20.4s/20471513699ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:19:55 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][8710][16] duration [20.4s], collections [1]/[26.1s], total [20.4s]/[22.5s], memory [110.1mb]->[61.1mb]/[512mb], all_pools {[young] [49mb]->[0b]/[0b]}{[old] [60.1mb]->[60.1mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.13 03:19:55 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][8710] overhead, spent [20.4s] collecting in the last [26.1s]
2022.05.13 03:23:46 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6dcabc02, interval=5s}] took [7192ms] which is above the warn threshold of [5000ms]
2022.05.13 03:23:38 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.1s/7192ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:24:29 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7.1s/7192009355ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:24:29 WARN  es[][o.e.t.ThreadPool] timer thread slept for [50.4s/50405ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:24:30 WARN  es[][o.e.t.ThreadPool] timer thread slept for [50.4s/50405583909ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:24:39 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.2s/8286ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:24:39 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.2s/8286635954ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:24:45 WARN  es[][o.e.h.AbstractHttpServerTransport] handling request [null][GET][/_cluster/health?master_timeout=30s&level=cluster&timeout=30s][Netty4HttpChannel{localAddress=/127.0.0.1:9001, remoteAddress=/127.0.0.1:33532}] took [5110ms] which is above the warn threshold of [5000ms]
2022.05.13 03:24:58 WARN  es[][o.e.m.f.FsHealthService] health check of [/opt/sonarqube/data/es7/nodes/0] took [16853ms] which is above the warn threshold of [5s]
2022.05.13 03:25:33 WARN  es[][o.e.t.ThreadPool] timer thread slept for [32.5s/32569ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:25:33 WARN  es[][o.e.t.ThreadPool] timer thread slept for [32.5s/32569556547ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:25:34 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [33047ms] which is above the warn threshold of [5000ms]
2022.05.13 03:26:21 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.7s/6772ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:26:21 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.7s/6771894906ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:26:27 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5015ms] which is above the warn threshold of [5000ms]
2022.05.13 03:26:39 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [5017ms] which is above the warn threshold of [5000ms]
2022.05.13 03:26:39 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5s/5018ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:02 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5s/5017557544ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:02 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][8983][17] duration [23.3s], collections [1]/[23.5s], total [23.3s]/[45.9s], memory [84.1mb]->[61.1mb]/[512mb], all_pools {[young] [23mb]->[0b]/[0b]}{[old] [60.1mb]->[60.1mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.13 03:27:02 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][8983] overhead, spent [23.3s] collecting in the last [23.5s]
2022.05.13 03:27:02 WARN  es[][o.e.t.ThreadPool] timer thread slept for [23.7s/23774ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:02 WARN  es[][o.e.t.ThreadPool] timer thread slept for [23.7s/23773975153ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:05 WARN  es[][o.e.m.f.FsHealthService] health check of [/opt/sonarqube/data/es7/nodes/0] took [26389ms] which is above the warn threshold of [5s]
2022.05.13 03:27:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.5s/8598ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.5s/8597722070ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:45 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.2s/8277ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:56 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.2s/8276918529ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:56 WARN  es[][o.e.t.ThreadPool] timer thread slept for [10.6s/10622ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:27:56 WARN  es[][o.e.t.ThreadPool] timer thread slept for [10.6s/10621417621ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:28:20 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [13266ms] which is above the warn threshold of [5000ms]
2022.05.13 03:29:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.3s/5354ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:29:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.3s/5353192044ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:30:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.2s/6296ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:30:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.2s/6295653062ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:30:03 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@6dcabc02, interval=5s}] took [6295ms] which is above the warn threshold of [5000ms]
2022.05.13 03:30:40 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7s/7022ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:30:40 WARN  es[][o.e.t.ThreadPool] timer thread slept for [7s/7022164845ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:30:53 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.9s/5930ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:30:53 WARN  es[][o.e.t.ThreadPool] timer thread slept for [5.9s/5929790593ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:31:15 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.2s/6201ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:31:15 WARN  es[][o.e.t.ThreadPool] timer thread slept for [6.2s/6200799824ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:32:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.2s/8292ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:32:57 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.2s/8292405459ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:33:46 WARN  es[][o.e.t.ThreadPool] timer thread slept for [14.1s/14163ms] on absolute clock which is above the warn threshold of [5000ms]
2022.05.13 03:33:46 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][9283][18] duration [8s], collections [1]/[1s], total [8s]/[54s], memory [84.1mb]->[84.1mb]/[512mb], all_pools {[young] [23mb]->[0b]/[0b]}{[old] [60.1mb]->[60.1mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.13 03:33:46 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][9283] overhead, spent [8s] collecting in the last [1s]
2022.05.13 03:33:46 WARN  es[][o.e.t.ThreadPool] timer thread slept for [14.1s/14163699665ns] on relative clock which is above the warn threshold of [5000ms]
2022.05.13 03:33:46 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@54cff991, interval=1s}] took [14163ms] which is above the warn threshold of [5000ms]
2022.05.13 03:28:26 INFO  es[][o.e.n.Node] version[7.17.1], pid[39], build[default/tar/e5acb99f822233d62d6444ce45a4543dc1c8059a/2022-02-23T22:20:54.153567231Z], OS[Linux/5.10.102.1-microsoft-standard-WSL2/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/11.0.14/11.0.14+9-alpine-r0]
2022.05.13 03:28:26 INFO  es[][o.e.n.Node] JVM home [/usr/lib/jvm/java-11-openjdk]
2022.05.13 03:28:26 INFO  es[][o.e.n.Node] JVM arguments [-XX:+UseG1GC, -Djava.io.tmpdir=/opt/sonarqube/temp, -XX:ErrorFile=../logs/es_hs_err_pid%p.log, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djna.tmpdir=/opt/sonarqube/temp, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=COMPAT, -Dcom.redhat.fips=false, -Xmx512m, -Xms512m, -XX:MaxDirectMemorySize=256m, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/sonarqube/elasticsearch, -Des.path.conf=/opt/sonarqube/temp/conf/es, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=false]
2022.05.13 03:28:26 INFO  es[][o.e.p.PluginsService] loaded module [analysis-common]
2022.05.13 03:28:26 INFO  es[][o.e.p.PluginsService] loaded module [lang-painless]
2022.05.13 03:28:26 INFO  es[][o.e.p.PluginsService] loaded module [parent-join]
2022.05.13 03:28:26 INFO  es[][o.e.p.PluginsService] loaded module [reindex]
2022.05.13 03:28:26 INFO  es[][o.e.p.PluginsService] loaded module [transport-netty4]
2022.05.13 03:28:26 INFO  es[][o.e.p.PluginsService] no plugins loaded
2022.05.13 03:28:26 INFO  es[][o.e.e.NodeEnvironment] using [1] data paths, mounts [[/opt/sonarqube/data (/dev/sdb)]], net usable_space [236.3gb], net total_space [250.9gb], types [ext4]
2022.05.13 03:28:26 INFO  es[][o.e.e.NodeEnvironment] heap size [512mb], compressed ordinary object pointers [true]
2022.05.13 03:28:26 INFO  es[][o.e.n.Node] node name [sonarqube], node ID [0KICZHGIR_WEdTCwjG6c4Q], cluster name [sonarqube], roles [data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
2022.05.13 03:28:29 INFO  es[][o.e.t.NettyAllocator] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=512mb}]
2022.05.13 03:28:29 INFO  es[][o.e.i.r.RecoverySettings] using rate limit [40mb] with [default=40mb, read=0b, write=0b, max=0b]
2022.05.13 03:28:29 INFO  es[][o.e.d.DiscoveryModule] using discovery type [zen] and seed hosts providers [settings]
2022.05.13 03:28:30 INFO  es[][o.e.g.DanglingIndicesState] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
2022.05.13 03:28:30 INFO  es[][o.e.n.Node] initialized
2022.05.13 03:28:30 INFO  es[][o.e.n.Node] starting ...
2022.05.13 03:28:30 INFO  es[][o.e.t.TransportService] publish_address {127.0.0.1:36863}, bound_addresses {127.0.0.1:36863}
2022.05.13 03:28:30 WARN  es[][o.e.b.BootstrapChecks] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
2022.05.13 03:28:30 INFO  es[][o.e.c.c.Coordinator] cluster UUID [ibnBxlt0TPG9C3JACSFhpQ]
2022.05.13 03:28:30 INFO  es[][o.e.c.s.MasterService] elected-as-master ([1] nodes joined)[{sonarqube}{0KICZHGIR_WEdTCwjG6c4Q}{1_hz-u0_QXWN7ROHjV18YA}{127.0.0.1}{127.0.0.1:36863}{cdfhimrsw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 5, version: 103, delta: master node changed {previous [], current [{sonarqube}{0KICZHGIR_WEdTCwjG6c4Q}{1_hz-u0_QXWN7ROHjV18YA}{127.0.0.1}{127.0.0.1:36863}{cdfhimrsw}]}
2022.05.13 03:28:30 INFO  es[][o.e.c.s.ClusterApplierService] master node changed {previous [], current [{sonarqube}{0KICZHGIR_WEdTCwjG6c4Q}{1_hz-u0_QXWN7ROHjV18YA}{127.0.0.1}{127.0.0.1:36863}{cdfhimrsw}]}, term: 5, version: 103, reason: Publication{term=5, version=103}
2022.05.13 03:28:30 INFO  es[][o.e.h.AbstractHttpServerTransport] publish_address {127.0.0.1:9001}, bound_addresses {127.0.0.1:9001}
2022.05.13 03:28:30 INFO  es[][o.e.n.Node] started
2022.05.13 03:28:30 INFO  es[][o.e.g.GatewayService] recovered [7] indices into cluster_state
2022.05.13 03:28:31 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[metadatas][0]]]).
2022.05.13 03:47:58 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5c74a10f, interval=1s}] took [6889ms] which is above the warn threshold of [5000ms]
2022.05.13 04:14:32 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][young][2692][10] duration [4.5s], collections [1]/[1s], total [4.5s]/[4.6s], memory [325.5mb]->[325.5mb]/[512mb], all_pools {[young] [264mb]->[264mb]/[0b]}{[old] [42.5mb]->[42.5mb]/[512mb]}{[survivor] [19mb]->[19mb]/[0b]}
2022.05.13 04:14:32 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][2692] overhead, spent [4.5s] collecting in the last [1s]
2022.05.13 04:14:32 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.monitor.jvm.JvmGcMonitorService$1@5c74a10f, interval=1s}] took [5008ms] which is above the warn threshold of [5000ms]
2022.05.13 04:46:24 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][4603] overhead, spent [438ms] collecting in the last [1s]
2022.05.13 05:32:15 INFO  es[][o.e.m.j.JvmGcMonitorService] [gc][young][7338][24] duration [937ms], collections [1]/[1.8s], total [937ms]/[6.1s], memory [107.9mb]->[61.9mb]/[512mb], all_pools {[young] [46mb]->[0b]/[0b]}{[old] [60.9mb]->[60.9mb]/[512mb]}{[survivor] [1mb]->[1mb]/[0b]}
2022.05.13 05:32:15 WARN  es[][o.e.m.j.JvmGcMonitorService] [gc][7338] overhead, spent [937ms] collecting in the last [1.8s]
